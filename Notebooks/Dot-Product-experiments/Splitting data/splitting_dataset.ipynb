{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","\n","from google.colab import drive"],"metadata":{"id":"Yx_U1RvzEJcA","executionInfo":{"status":"ok","timestamp":1700742710854,"user_tz":-120,"elapsed":438,"user":{"displayName":"Schadrack NIYIBIZI","userId":"09624253821683112798"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["## Some constant variables\n","drive.mount('/content/drive')\n","CAPSTONE_PATH = \"/content/drive/MyDrive/CMU/Fall 2023/Capstone Project - Team Cylab/CapStone/\"  # replace it with your path to the drive for capstone shared with you\n","DATA_ROOT_PATH = os.path.join(CAPSTONE_PATH, 'data')\n","DATA_UNSEEN_PATH = os.path.join(DATA_ROOT_PATH, \"UNSEEN_DATA\")\n","\n","# Comment and uncomment below two lines to toggle different datasets\n","data_dir = os.path.join(DATA_ROOT_PATH, 'LOW_QUALITY_WITH_OG')  # Final dataset with both Original dataser and low quality generated dataset\n","data_unseen_all = os.path.join(DATA_UNSEEN_PATH, \"ALL\")\n","# data_dir = os.path.join(DATA_ROOT_PATH, 'LOW_QUALITY_WITH_NO_OG')  # Final low quality dataset without original images\n","\n","# Define the paths for your dataset\n","# train_dir = os.path.join(DATA_ROOT_PATH, 'SPLIT_DATA_WITH_NO_OG', 'Train')\n","# val_dir = os.path.join(DATA_ROOT_PATH, 'SPLIT_DATA_WITH_NO_OG', 'Valid')\n","# test_dir = os.path.join(DATA_ROOT_PATH, 'SPLIT_DATA_WITH_NO_OG', 'Test')\n","\n","\n","# Define the paths for your dataset\n","train_dir = os.path.join(DATA_ROOT_PATH, 'SPLIT_DATA_WITH_OG', 'Train')\n","val_dir = os.path.join(DATA_ROOT_PATH, 'SPLIT_DATA_WITH_OG', 'Valid')\n","test_dir = os.path.join(DATA_ROOT_PATH, 'SPLIT_DATA_WITH_OG', 'Test')\n","feature_extractor_dir = os.path.join(DATA_ROOT_PATH, 'SPLIT_DATA_WITH_OG', 'Feature_Extractor')\n","\n","\n","\n","test_unseen_dir = os.path.join(DATA_UNSEEN_PATH, \"TEST\")\n","# feature_extractor_dir = os.path.join(DATA_UNSEEN_PATH, \"FEATURE_EXTRACTOR\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rdhYSBzFEQx2","executionInfo":{"status":"ok","timestamp":1700742751317,"user_tz":-120,"elapsed":3425,"user":{"displayName":"Schadrack NIYIBIZI","userId":"09624253821683112798"}},"outputId":"93a187fc-3295-48aa-a167-6315f81ffca8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gwnumAwTD5Bs"},"outputs":[],"source":["def split_unseen_dataset():\n","    \"\"\"\n","    The function which splits the dataset into train, validation and test\n","    \"\"\"\n","    os.makedirs(test_unseen_dir, exist_ok=True)\n","    os.makedirs(feature_extractor_dir, exist_ok=True)\n","    os.makedirs(data_unseen_all, exist_ok=True)\n","\n","    # Define the split ratios\n","    train_ratio = 0.7 # 70% for training\n","    val_ratio = 0.15  # 15% for validation\n","    test_ratio = 0.15  # 15% for testing\n","\n","    # List all subdirectories (each subdirectory corresponds to a class)\n","    class_dirs = [d for d in os.listdir(data_unseen_all) if os.path.isdir(os.path.join(data_unseen_all, d))]\n","\n","    # Iterate through each class directory and split the data\n","    for class_dir in class_dirs:\n","        class_data_dir = os.path.join(data_unseen_all, class_dir)\n","\n","        class_val_dir = os.path.join(feature_extractor_dir, class_dir)\n","        class_test_dir = os.path.join(test_unseen_dir, class_dir)\n","\n","        os.makedirs(class_val_dir, exist_ok=True)\n","        os.makedirs(class_test_dir, exist_ok=True)\n","\n","        class_files = os.listdir(class_data_dir)\n","        random.shuffle(class_files)\n","\n","        num_files = len(class_files)\n","        num_train = int(train_ratio * num_files)\n","        num_val = int(val_ratio * num_files)\n","\n","        train_files = class_files[:num_train]\n","        val_files = class_files[num_train:num_train + num_val]\n","        test_files = class_files[num_train + num_val:]\n","\n","        # for file in train_files:\n","        #     src = os.path.join(class_data_dir, file)\n","        #     dst = os.path.join(class_train_dir, file)\n","        #     shutil.copy(src, dst)\n","\n","        for file in val_files:\n","            src = os.path.join(class_data_dir, file)\n","            dst = os.path.join(class_val_dir, file)\n","            shutil.copy(src, dst)\n","\n","        for file in test_files:\n","            src = os.path.join(class_data_dir, file)\n","            dst = os.path.join(class_test_dir, file)\n","            shutil.copy(src, dst)\n"]},{"cell_type":"code","source":["def split_dataset(data_dir):\n","    \"\"\"\n","    The function which splits the dataset into train, validation and test\n","    \"\"\"\n","\n","\n","    # Create the train, validation, and test directories if they don't exist\n","    os.makedirs(train_dir, exist_ok=True)\n","    os.makedirs(val_dir, exist_ok=True)\n","    os.makedirs(feature_extractor_dir, exist_ok=True)\n","    os.makedirs(test_dir, exist_ok=True)\n","\n","    # Define the split ratios\n","    # Define the split ratios\n","    train_ratio = 0.6 # 70% for training\n","    val_ratio = 0.15  # 15% for validation\n","    feature_extractor = 0.15\n","    test_ratio = 0.10  # 15% for testing\n","\n","    # List all subdirectories (each subdirectory corresponds to a class)\n","    class_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n","\n","    # Iterate through each class directory and split the data\n","    for class_dir in class_dirs:\n","        class_data_dir = os.path.join(data_dir, class_dir)\n","        class_train_dir = os.path.join(train_dir, class_dir)\n","        class_val_dir = os.path.join(val_dir, class_dir)\n","        class_feature_extractor_dir = os.path.join(feature_extractor_dir, class_dir)\n","        class_test_dir = os.path.join(test_dir, class_dir)\n","\n","        os.makedirs(class_train_dir, exist_ok=True)\n","        os.makedirs(class_val_dir, exist_ok=True)\n","        os.makedirs(class_feature_extractor_dir, exist_ok=True)\n","        os.makedirs(class_test_dir, exist_ok=True)\n","\n","        class_files = os.listdir(class_data_dir)\n","        random.shuffle(class_files)\n","\n","        num_files = len(class_files)\n","        num_train = int(train_ratio * num_files)\n","        num_val = round(val_ratio * num_files)\n","        num_feature_extractor = round(feature_extractor * num_files)\n","        num_test = int(test_ratio * num_files)\n","\n","        train_files = class_files[:num_train]\n","        val_files = class_files[num_train:num_train + num_val]\n","        feature_extractor_files = class_files[num_train + num_val:num_train + num_val + num_feature_extractor]\n","        test_files = class_files[num_train + num_val + num_feature_extractor:]\n","\n","        for file in train_files:\n","            src = os.path.join(class_data_dir, file)\n","            dst = os.path.join(class_train_dir, file)\n","            shutil.copy(src, dst)\n","\n","        for file in val_files:\n","            src = os.path.join(class_data_dir, file)\n","            dst = os.path.join(class_val_dir, file)\n","            shutil.copy(src, dst)\n","\n","        for file in feature_extractor_files:\n","            src = os.path.join(class_data_dir, file)\n","            dst = os.path.join(class_feature_extractor_dir, file)\n","            shutil.copy(src, dst)\n","\n","        for file in test_files:\n","            src = os.path.join(class_data_dir, file)\n","            dst = os.path.join(class_test_dir, file)\n","            shutil.copy(src, dst)\n"],"metadata":{"id":"slYPPDMSHwK9","executionInfo":{"status":"ok","timestamp":1700743008951,"user_tz":-120,"elapsed":515,"user":{"displayName":"Schadrack NIYIBIZI","userId":"09624253821683112798"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["split_unseen_dataset()"],"metadata":{"id":"N2ezA2qOrTyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["split_dataset(data_dir)"],"metadata":{"id":"8A6y25jZrW6u","executionInfo":{"status":"ok","timestamp":1700747211313,"user_tz":-120,"elapsed":4198682,"user":{"displayName":"Schadrack NIYIBIZI","userId":"09624253821683112798"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iUJ758cmXxf4"},"execution_count":null,"outputs":[]}]}